---
title: "Proyecto Médodos II"
author: "Mauricio Caicedo Palacio - Fidela Makendengue"
date: "2022-01-30"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: inline
---

# Introducción

En el siguiente trabajo se hace un análisis de componentes principales, análisis factorial y análisis de conglomerados. Se considera la base de datos Heart Disease Diagnosis. Esta base de datos contiene 4 bases de datos relacionadas con el diagnóstico de enfermedades del corazón. Todos los atributos tienen valores numéricos. Los datos fueron recolectados de las cuatro ubicaciones siguientes:

-   Fundación de la Clínica Cleveland (cleveland.data)

-   Instituto Húngaro de Cardiología, Budapest (hungarian.data)

-   VA Centro médico, Long Beach, CA (long-beach-va.data)

-   Hospital Universitario, Zúrich, Suiza (switzerland.data)

En particular, nosotros utilizaremos la base de datos de Cleveland, donde la variable objetivo se refiere a la presencia de enfermedad cardíaca en el paciente. Tiene un valor entero de 0 a 4, donde cero es sin presencia de la enfermedad. Los experimentos con la base de datos de Cleveland se han concentrado simplemente en intentar distinguir la presencia (valores 1,2,3,4) de la ausencia (valor 0).

Las bases de datos tienen 76 atributos sin procesar, sin embargo solo se proporcionan 14 de ellos, cuando se descarga de la url, dado que estos son los que se utilizan para estudios relacionados con Machine Learning.

Los autores de las bases de datos son: 1. Instituto Húngaro de Cardiologia. Budapest: Andras Janosi, MD 2. Hospital Universitario, Zúrich, Suiza: William Steinbrunn, M.D. 3. Hospital Universitario, Basilea, Suiza: Matthias Pfisterer, M.D. 4. VA Centro Médico, Long Beach y Fundación de la Clínica Cleveland: Robert Detrano, MD, Ph.D.

Finalmente, cabe mencionar que en este trabajo se presenta: un análisis previo, un análisis de componentes principales, un análisis factorial y un análisis de conglomerados.

# Análisis previo

## Resúmenes

Las bases de datos y la información relacionada con las mismas, se pueden encontrar en el siguiente enlace:

<https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/>

Ahora bien, veamos la información de los nombres de los atributos o las columnas:

1\. age: edad en años.

2\. sex: sexo (1 = masculino; 0 = femenino).

3\. cp: tipo de dolor en el pecho \-- Value 1: angina típica \-- Value 2: angina atípica \-- Value 3: dolor sin angina \-- Value 4: asintomático.

4\. trestbps: presión arterial en reposo (en mm Hg al ingreso en el hospital)

5\. chol: colesterol sérico en mg/dl

6\. fbs: (azúcar en sangre en ayunas \\\> 120 mg/dl) (1 = verdadero; 0 = falso).

7\. restecg: resultados electrocardiográficos en reposo \-- Valor 0: normal \-- Valor 1: tener anormalidad de onda ST-T (inversiones de onda T y/o ST elevación o depresión de \\\> 0,05 mV) \-- Valor 2: mostrar hipertrofia ventricular izquierda probable o definitiva según el criterio Estes

8\. thalach: frecuencia cardíaca máxima alcanzada.

9\. exang: angina inducida por el ejercicio (1 = sí; 0 = no).

10\. oldpeak: Depresión del ST inducida por el ejercicio en relación con el reposo.

11.slope: pendiente del segmento ST de ejercicio máximo \-- Valor 1: ascendente \-- Valor 2: plano \-- Valor 3: descendente.

12\. ca: número de vasos principales (0-3) coloreados por fluoroscopia.

13\. thal: 3 = normales; 6 = defecto fijo; 7 = defecto reversible.

14\. num: diagnóstico de enfermedad cardíaca (estado de enfermedad angiográfico) \-- Valor 0: \\\< 50% estrechamiento del diámetro \-- Valor 1: \\\> 50% estrechamiento del diámetro (en cualquier vaso principal: los atributos 59 a 68 son vasos).

```{r}
#Cargamos las librerías que vamos a utilizar para todo el trabajo
library(GGally)
library(ggplot2)
library(DescTools)
library(binom)
library(nortest)
library(stringr)
library(NbClust)
library(rgl)
library(psych)
library(factoextra)
library(caret)
library(GGally)
library(patchwork)
library(Stat2Data)
library(corrplot)
```

Veamos una descripción sobre el conjunto de datos:

```{r}
#Cargamos los datos
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data",sep = ",")

#Cambiamos el nombre de las columnas 
colnames(data) <- c('age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num')

#Visualizamos las primeras 5 instancias de los datos.
head(data,5)
```

```{r}
tail(data, 3)
```

En las visualizaciones de los datos se observan valores pequeños desde 0,1,2 hasta 250, 286, de acuerdo a las primeras tres filas.Como también encontramos datos enteros y punto flotante. Veamos la dimensión del conjunto de datos:

```{r}
dim(data)
```

Este conjunto de datos contiene la información de 302 pacientes (302 instancias) basados en 14 atributos. A continuación, realizamos un breve preprocesado de datos, para ello analizamos si el conjunto de datos contiene datos nulos.

```{r}
any(is.na(data))
```

En efecto, el conjunto de datos no contiene datos nulos, ahora bien, vamos a hacer el estudio de los datos a nivel estadístico, y vamos a denotar a la variable data_num al conjunto de los datos numéricos sin incluir las variables ca y thal, dado que son categóricas, de esta modo, trabajamos sólo con 12 atributos.

```{r}
#Seleccionamos las variables numéricas sin la variable ca y thal
nums <- unlist(lapply(data, is.numeric), use.names = FALSE) 
data_num <- data[ , nums]
head(data_num, 5)
```

Resumen global del data frame.

```{r}
resumen <- apply(data_num, 1,
                     function(x) c(mean(x),sd(x),quantile(x)))
rownames(resumen)<- c("Media","DT","Mínimo","Q1","Q2","Q3","Máximo")
knitr::kable(resumen, digits = 5, align = "c", caption="Resumen global de las variables numéricas")
```

## Visualización

En la siguiente sección, realizamos algunas gráficas para determinar relaciones entre las variables, distribuciones de los datos, entre otras características que iremos detallando. También, usaremos la notación HDD para denotar el conjunto de datos Heart Disease Diagnosis.

El primero es el gráfico de correlaciones

```{r echo=TRUE}
#Gráfico de correlaciones
title <- "Correlaciones entre las variables de HDD"
#Matriz de correlaciones
M <- cor(data_num)
corrplot(M, col = COL2('RdYlBu'), addCoef.col = 'black', tl.pos = 'd')
```

```{r}
corPlot(data_num, min.length = 9, cex = 0.5, alpha = 0.8, colors = TRUE )
```

Para comprender el gráfico de correlaciones, debemos tener en cuenta que si, el valor que toma es 1, quiere decir que las variables están completamente relacionadas, además el color azul más fuerte significa lo mismo, en tonalidades más claras tenemos una relación menor. Por otro lado, diremos que dos variables no están relacionadas cuando toman valores de -1, también el color rojo representa que no hay relación entre estas variables, donde el rojo más claro como rosa determina un leve incremento de la relación. El color blanco, cuyo valor asociado es el cero, indica que hay un 50% de relación entre las variables. Algunas inferencias del gráfico anterior son:

1\. Las variables que están 100% relacionadas son cada una con ella misma.

2\. Las variables más relacionadas son: slope con oldpeak (0.58), num con oldpeak(0.51), num con cp (0.41), num con slope (0.39), exang con cop (0.38).

3\. Las variables menos relacionadas son: thalac con num (-42), thalac con age (-0.39), thalac con slope (-0.39), thalac con exang (-0.38), thalach con oldpeak (-0.34), thalac con oldpeak (-0.34).

4\. La variable thalac es la que está menos relacionada con las variables: num, slope, oldpeak, exang, age, cp. Podemos pensar en eliminar esta variable del conjunto de datos.

Como consecuencia, se elimina la variable thalac y seguimos el estudio con 11 atributos.

```{r}
#Seleccionamos las variables numéricas sin la variable ca, thal y thalac

data_num <- data_num[,-8] # 8 es el índice de la variable thalac
head(data_num, 4)
```

A continuación, analizamos la correlación entre las variables mediante las distancias, mediante la métrica del máximum. Para su visualización utilizamos un mapa de calor con 15 instancias. Sólo visualizamos las primeras 2 filas

```{r}
# Matriz de distancias
distancias <- as.matrix(dist(data_num, method = 'maximum'))
head(distancias,2)
```

Pero no se interpretan muy bien estos valores, para ello, realizamos un mapa de calor para 15 instancias con la métrica de pearson.

```{r}
# Correlación basada en el método de la distancia
res.dist <- get_dist(data_num[1:15,], method = "pearson")

# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 5)
```

En este gráfico se representan de color azul las distancias más grandes, siendo el valor máximo 0.12 y el mínimo 0.0 con el color rosa, no obstante podemos observar que son más los valores cuyas distancias son mínimas respecto a los valores con mayor distancias.

Teniendo en cuenta que en el conjunto de datos encontramos valores desde 0 a 259, para tener una estandarización de los mismos, consideramos escalar el conjunto de datos, donde cada atributo tiene media 0 y desviación típica de 1. No escalamos la variable objetivo num. Por otro lado, vamos a cambiar el nombre de la variable objetivo de num a 'diagnosis'.

```{r}
#Escalamos el conjunto de datos sin las variables: ca, thal, thalach, num.
#tem <- data_num[,-12]
#tem <- data_num[,-8]
data_scale <- scale(data_num[,-11]) # 11 es el índice de la variable objetivo num
head(data_scale,3)
```

```{r}
#Vector con datos escalados
vector1 <- data_scale

#Vector de la variable objetivo
vector2 <- data$num

# Combinamos los dos vectores usando cbind()
data1 <- cbind(vector1,vector2)
```

```{r}
head(data1, 2)
```

```{r echo=TRUE}
# Aplicamos los nombres a las columnas
colnames(data1) <- c('age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',
                    'restecg', 'exang','oldpeak', 'slope','diagnosis')

#Lo convertimos en un dataframe
data1 <- as.data.frame(data1)
head(data1, 3)
```

Si analizamos los valores del atributo 'diagnosis', podemos ver que toma un valor de 2, pero de acuerdo a la información del conjunto de datos, esta variable sólo puede tomar valores de 0 y 1. Veamos qué valores numéricos tiene este atributo

```{r echo=TRUE}
freq <- as.data.frame(table(data1$diagnosis))
freq
```

Basado en la tabla anterior, la variable 'diagnosis' toma valores de 0,1,2,3 y 4. En consecuencia, vamos a remover los datos con valores de 2,3 y 4. Ahora bien, una vez terminado esto, vamos a clasificar los datos de 0 como 'healthy' y de 1 como 'heartDisease'.

```{r}
data1 <- data1[(data1$diagnosis=="0" | data1$diagnosis=="1"),]
head(data1)
```

Podemos ver que la variable diagnosis toma valores de 0 y 1 como esperábamos. Ahora la dimensión del conjunto de datos es 218 instancias y 11 columnas.

```{r}
dim(data1)
```

Por lo que hemos reducido el número de instancias de 302 a 218. Para una mejor comprensión de los datos, vamos a

```{r}
# Reasignamos los diagnósticos como healty y heartDisease
data1$diagnosis[data1$diagnosis == 0] <- "healthy"
data1$diagnosis[data1$diagnosis == 1] <- "heartDisease"

#Visualizamos los datos de las 3 primeras columnas
head(data1,3)
```

A partir de ahora nuestros datos están estandarizados.

```{r}
data2 <- as.data.frame(data1)

#Activamos las variables del dataset para llamarlas sin recurrir al conjunto de datos "data2"
attach(data2)
head(data2)
```

También podemos visualizar mediante un gráfico de pares (pair-plot), las relaciones de los datos escalados de acuerdo a su diagnóstico

```{r}
# Pair-plot de las características
ggpairs(data2[,1:5], aes(colour=factor(data2$diagnosis), alpha=0.2)) 
```

```{r}
# Pair-plot de las características
ggpairs(data2[,6:10], aes(colour=factor(data2$diagnosis), alpha=0.2))
```

```{r warning=FALSE}
ggplot(data2, 
       aes(x = age, 
           y=chol)) +
geom_point()
```

Antes de continuar con el análisis, con los datos estandarizados, vale la pena observar la relación entre la edad y los niveles de azúcar en la sangre. Lo anterior se ve mejor sin la normalización.

```{r}
ggplot(data_num, 
       aes(x = age, 
           y=chol)) +
geom_point()
```

Por otro lado, en general las personas con problemas del corazón tienen una edad más avanzada.

```{r}
p0 <- ggplot(data2, aes(x = age, fill = diagnosis)) +
geom_histogram(binwidth = .5)
p1 <- ggplot(data2, aes(diagnosis, age)) + geom_boxplot()
p0 + p1
```

```{r}
p2 <- ggplot(data2, aes(x = cp, fill = diagnosis)) +
geom_histogram(binwidth = .5)
p3 <- ggplot(data2, aes(diagnosis, cp)) + geom_boxplot()
p2 + p3
```

## Modelado usando submuestras

Definimos la submuestra del conjunto de datos. Examinaremos la capacidad predictiva de un modelo, para ello, vamos a utilizar un 70% de los objetos para entrenar y un 30% para evaluar dicho modelo.

```{r}
n <- dim(data2)[1]
set.seed(12)
indices_aprendizaje <- sample(n, round(.7*n))
data_aprendizaje <- data2[indices_aprendizaje, ]
head(data_aprendizaje)
```

Veamos el conjunto de test:

```{r}
data_test <- data2[-indices_aprendizaje, ]
head(data_test)
```

Ya tenemos las submuestras, a partir de aquí vamos a entrenar un modelo y posteriormente testar. Para empezar vamos a utilizar un modelo de regresión lineal múltiple que explique la salud del corazón en función del resto de la variables.

Ajustamos el modelo usando los datos de entrenamiento.

```{r}
na.omit(data_aprendizaje)
```

```{r}
unique(data_aprendizaje$diagnosis)
```

```{r}
data_aprendizaje <- na.omit(data_aprendizaje)
data_aprendizaje$diagnosis[data_aprendizaje$diagnosis == "healthy"] <- 0 
data_aprendizaje$diagnosis[data_aprendizaje$diagnosis == "heartDisease"] <- 1

modelo <- lm(diagnosis ~ ., data=data_aprendizaje)
summary(modelo)
```

Las variables más relevantes son: sex, cp, exang y slope. Podría considerar, realizar un modelado con estás variables, y comparar el resultado con la anterior, veamos.

```{r}
modelo2 <- lm(diagnosis ~ sex + cp + exang + slope + chol, data=data_aprendizaje)
summary(modelo2)
```

El r cuadrado ajustado es un poco mejor que usando todas las variables disponibles. Veamos un poco más la validación de estos modelos.

```{r}
# Q-Q plot
qqnorm(rstandard(modelo))
qqline(rstandard(modelo), lty = 2)

# frente a valores ajustados: comprobar que la nube sea dispersa
plot(rstandard(modelo) ~ fitted(modelo))
abline(h = 0, lty = 2)
abline(h = c(-2, 2), lty = 3, col = 4)

# Comprobar si se rechaza la normalidad de los residuos
shapiro.test(rstandard(modelo))
# Varios gráficos de validación, incluye alguno de los resultados anteriores
plot(modelo)
# Lo realizado hasta aquí nos podría servir también para validar un modelo ANOVA,
# por ejemplo para: 'modelo <- aov(Sepal.Length ~ Species, data = iris)'
# Comprobar si son dispersas las nubes de los residuos frente a las covariables
par(mfrow = c(1, 2))
plot(resid(modelo) ~ data_aprendizaje$diagnosis)
plot(resid(modelo) ~ data_aprendizaje$diagnosis)
par(mfrow = c(1, 1))
```

```{r}
qqnorm(rstandard(modelo2))
qqline(rstandard(modelo2), lty = 2)

plot(rstandard(modelo2) ~ fitted(modelo2))
abline(h = 0, lty = 2)
abline(h = c(-2, 2), lty = 3, col = 4)

# Comprobar si se rechaza la normalidad de los residuos
shapiro.test(rstandard(modelo2))
# Varios gráficos de validación, incluye alguno de los resultados anteriores
plot(modelo2)
# Lo realizado hasta aquí nos podría servir también para validar un modelo ANOVA,
# por ejemplo para: 'modelo <- aov(Sepal.Length ~ Species, data = iris)'
# Comprobar si son dispersas las nubes de los residuos frente a las covariables
par(mfrow = c(1, 2))
plot(resid(modelo2) ~ data_aprendizaje$diagnosis)
plot(resid(modelo2) ~ data_aprendizaje$diagnosis)
par(mfrow = c(1, 1))
```

# Análisis de componentes principales

Tiendo en cuenta que el segundo modelo podría funcionar mejor que el primero, teniendo menos variables, vamos a realizar un análisis de las componentes principales en el dataset. Este análisis nos permite entender la varianza en función de la cantidad de variables.

```{r}
head(data2, 4)
```

```{r}
X <- as.matrix(data2[, -11])
p <- ncol(X)
S <- cov(X)
var_tot <- sum(diag(S))
var_tot
```

```{r}
acp_cov <- prcomp(data2[,-11])
acp_cov
summary(acp_cov)
plot(acp_cov, type = "l", pch = 19)
```

No es claro que hay un codo, es decir, no hay un punto donde la varianza deja de decrecer considerablemente. Veamos como se ven las primeras dos componentes.

```{r}
head(acp_cov$x)
cov(acp_cov$x[, 1:2])
```

```{r}
# Para interpretar
acp_cov$rotation[, 1:2]
cor(data2[, -11], acp_cov$x[, 1:2])
```

```{r}
diag <- cbind(data2, acp_cov$x[, 1:2])
```

```{r}
ggplot(diag, aes(x = PC1, y = data2$diagnosis)) + geom_point()
```

Con una componente no es clara la utilidad de las componentes.

```{r}
ggplot(diag, aes(x = PC1, y = PC2, color = data2$diagnosis)) + geom_point()
```

Dos componentes no permiten ver claramente la diferencia entre las dos categorías. Por otro lado, el biplot nos sirve para resumir las relaciones entre variables, entre objetos y entre variables y objetos.

```{r fig.height=6, fig.width=6}
biplot(acp_cov, scale = 0, cex = 0.6, col = c("blue4", "brown3"))
```

En este caso la dispersión de las variables no permite ver las diferentes contribuciones a las primeras dos componentes principales. Esto podría deberse a la poca diferencia de varianza entre las variables.

```{r}
plot(acp_cov, main="Varianza de las componentes")
```

Podríamos ver de manera más clara como se está acumulando la varianza en las componentes principales.

```{r}
prop_varianza <- acp_cov$sdev^2/sum(acp_cov$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)
ggplot(data = data.frame(prop_varianza_acum, pc = factor(1:10)),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum,2))) +
  theme_bw() +
  labs(x = "Componentes principales", 
       y = "Prop. varianza explicada acumulada")
```

Debido a la poca acumulación de varianza en las primeras componentes, se tiene mas de un cincuenta por ciento de la varianza en las primeras cuatro componentes principales.

# Análisis factorial

El objetivo de este análisis será identificar las variables más relevantes que influyen en la presencia de enfermedades cardíacas en los pacientes. Para ello, utilizaremos técnicas de extracción de factores para encontrar patrones en las variables y reducir el número de dimensiones a unas pocas variables latentes que expliquen la mayor cantidad de variabilidad en los datos.

Una vez identificados los factores, trataremos de utilizar esta información para entender mejor la relación entre las diferentes variables y cómo influyen en la presencia de enfermedades cardíacas. Este análisis puede proporcionar información valiosa para mejorar la prevención de las enfermedades cardíacas en la población en general.

```{r}
X <- data2[, -11]
p <- ncol(X)
R <- cor(X)
R
```

```{r}
#Valores-vectores propios.
Rprop <- eigen(R)
Rprop
```

Porcentajes acumulados de varianza.

```{r}
100*cumsum(Rprop$values)/p
```

Al igual que en los componentes principales, para tener más de cincuenta por ciento de la varianza hay que tomar los primeros cuatro factores. Ahora, elegimos el número de factores y obtenemos la estimación de la matriz de cargas.

```{r}
m <- 3
sum(Rprop$values[(m + 1):p]) # cota de la suma de cuadrados de la matriz residual
L <- (rep(1, p) %*% t(sqrt(Rprop$values[1:m]))) * Rprop$vectors[, 1:m]
```

```{r}
h2 <- rowSums(L^2)
h2
```

```{r}
psi <- 1 - h2
psi
```

Estimamos la matriz de correlaciones.

```{r}
Rbarra <- L %*%t(L) + diag(psi)
Rbarra
```

Se puede observar que algunas variables están moderadamente correlacionadas, mientras que otras tienen una correlación más débil. Por ejemplo, la presión arterial (trestbps) y el colesterol (chol) tienen una correlación moderada positiva de 0.26, mientras que el sexo (sex) y y el colesterol (chol) tienen una correlación negativa moderada de -0.43. La correlación más fuerte es de 0.58, que se da entre la variable que mide la lesión al miocardio (oldpeak: Depresión del ST) y la pendiente del segmento ST (slope). Veamos la matriz residual.

```{r}
head(data2,3)
```

```{r}
D <- R - Rbarra
sum(D^2)
```

Ahora, la representación gráfica de las cargas en el espacio de los factores.

```{r}
L <- as.data.frame(L)
rownames(L) <- colnames(X)
colnames(L) <- c("F1", "F2")
ggplot(L, aes(x = F1, y = F2, label = rownames(L))) +
geom_hline(yintercept = 0, lty = 2) +
geom_vline(xintercept = 0, lty = 2) +
geom_text()
```

No es claro que algunas variables carguen mas en uno de los factores, con excepción la variable sexo, parece tener una carga considerable en el factor dos.

Realizamos una rotación de factores:

```{r}
vmax_L <- varimax(as.matrix(L))
vmax_L
```

Realizamos una representación gráfica de las cargas en el espacio de los factores rotados.

```{r}
Lstar <- as.data.frame(vmax_L$loadings[, 1:m])
rownames(Lstar) <- colnames(X)
colnames(Lstar) <- c("F1", "F2")
ggplot(Lstar, aes(x = F1, y = F2, label = rownames(Lstar))) +
geom_hline(yintercept = 0, lty = 2) +
geom_vline(xintercept = 0, lty = 2) +
geom_text()
```

Con los factores rotados se ve una incidencia más fuerte de la depresión del ST inducida por el ejercicio en relación con el reposo (oldpeak), angina inducida por el ejercicio (exang), pendiente del segmento ST (slope) y en menor medida, el tipo de dolor en el pecho (cp) y el azúcar en sangre en ayunas (fbs).
Por otro lado veamos que, sin rotaciones funciona mejor el análisis y aumentar el número de factores no mejora de manera considerable.

```{r}
diagnosis_af <- principal(data2[, -(11)], nfactors = 4)
diagnosis_af
```

```{r}
fa.diagram(diagnosis_af)
```

Si tomamos cuatro componentes, en la primera aparecen la depresión del ST inducida por el ejercicio (oldpeak) y la pendiente del segmento ST (slope), que como y mencionamos estás muy correlacionadas. Pero en general no permiten ver una incidencia de ciertas variables en los factores.

# Análisis de conglomerados

El análisis de conglomerados es una técnica de modelado de datos que se utiliza para agrupar objetos similares en grupos o clusters.

## Kmeans

Entre los métodos de análisis de conglomerados, el algoritmo k-means es uno de los más populares y amplia mente utilizados. El algoritmo k-means es una técnica de particionamiento que busca dividir los datos en k clústeres, de tal manera que las observaciones dentro de cada clúster sean lo más similares posible entre sí y lo más diferentes posible de las observaciones en otros clústeres. En esta sección, se explorará el uso del algoritmo k-means. Seleccionamos K = 2 casos como centroides iniciales ya que se conoce que hay dos categorías.

```{r}
head(data2, 3)
```

```{r}
datos_km <- kmeans(data2[, 1:10], 2, nstart = 1e3)
datos_km
datos_km$tot.withinss
datos_km$withinss
sum(datos_km$withinss)

```

Es importante asegurarse de que el algoritmo k-means esté agrupando los datos de una manera que sea coherente con la estructura real de los datos. En otras palabras, el objetivo de utilizar k-means no es simplemente obtener una partición de los datos, sino también garantizar que los grupos resultantes sean relevantes

```{r}
ggplot(data2[, 1:10],
col = datos_km$cluster + 1,
pch = as.numeric(as.factor(data2$diagnosis)))
table(datos_km$cluster, data2$diagnosis)
```

En algunos casos, puede no estar claro si el algoritmo k-means está agrupando los datos de una manera que sea consistente con la estructura real de los datos. Esto puede suceder si los datos presentan una estructura compleja o si las variables no están adecuadamente normalizadas. En este caso, es posible que el algoritmo k-means produzca clústeres que no reflejen la verdadera estructura subyacente de los datos, ya que está des-balanceado el dataset.

```{r}
ncluster <- 1:20
distorsion <- sapply(ncluster,
function(K) kmeans(data2[,-11],
K, nstart = 10)$tot.withinss)

plot(ncluster,
distorsion,
type = "b",
pch = 19,
xlab = "K",
ylab = "Distorsión")
```

## Clustering jerárquico agregativo

Los métodos jerárquicos agregativos son una técnica de análisis multivariante que permiten la agrupación de observaciones en clústeres. Estos métodos se basan en la idea de que las observaciones más similares deben agruparse juntas y las menos similares deben separarse. En la práctica, este método comienza con cada observación en su propio grupo y, a continuación, combinan gradualmente los grupos más similares hasta que se alcanza un único grupo que contiene todas las instancias u observaciones. Este proceso de agrupamiento se representa mediante un dendrograma, que es una representación gráfica de la estructura jerárquica de los grupos, nuestro caso no mostramos está representación por la cantidad de datos. 

```{r}
data_NCCo <- NbClust(data2[,-11], method = "complete")
data_NCS <- NbClust(data2[,-11], method = "single")
data_NCA <- NbClust(data2[,-11], method = "average")
data_NCCe <- NbClust(data2[,-11], method = "centroid")
```

```{r}
#data_NCA
data_NCA$Best.partition
```

El mejor cluster identifica que el dataset presenta sesgo hacia una variable en particular.

```{r}
data_pca <- prcomp(data2[,-11])
Grupo <- as.factor(
data_NCA$Best.partition
)
ggplot(as.data.frame(data_pca$x),
aes(x = PC1, y = PC2, color = Grupo,
label = rownames(data2))) +
geom_hline(yintercept = 0, lty = 2) +
geom_vline(xintercept = 0, lty = 2) +
geom_text()
```

Básicamente está realizando un solo grupo. La técnica funcionaría mejor si balanceamos los datos.

# Bibliografía

[1] W. Larry, All of Statistics, 1st ed. Springer Science, 2004.
[2] R Core Team, 'R: A Language and Environment for Statistical Computing'. Vienna, Austria, 2018.
[Online]. Available: <https://www.R-project.org/>
[3] Rs. Team, 'RStudio: Integrated Development Environment for R'. RStudio, Inc., 2022.
[4] Higueras. H. Manuel, Notas de Clase. 2022.
[5] E. Brian, H. Torsten, An Introduction to Applied Multivariate Analysis with R, 1st ed. Springer Science, 2011.
[6] P. Daniel, Análisis de Datos Multivariantes, 2014.
